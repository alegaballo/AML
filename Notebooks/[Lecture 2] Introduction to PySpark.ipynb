{"nbformat_minor": 0, "nbformat": 4, "cells": [{"source": ["<div>\n", "<h1>Run the cell below to generate the road map (do not modify it)</h1></div>"], "cell_type": "markdown", "metadata": {}}, {"execution_count": 1, "cell_type": "code", "source": ["%%javascript\n", "var kernel = IPython.notebook.kernel;var thename = window.document.getElementById(\"notebook_name\").innerHTML;var command = \"THE_NOTEBOOK = \" + \"'\"+thename+\"'\";kernel.execute(command);command=\"os.environ['THE_NOTEBOOK'] = THE_NOTEBOOK\";kernel.execute(command);var cell = IPython.notebook.get_cell(2);cell.execute();IPython.notebook.get_cell(3).focus_cell();var x = $('.code_cell');$(x[1]).children('.input').hide();"], "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 2, "cell_type": "code", "source": ["outputdir = \"/tmp/tools/\"\n", "!mkdir -p $outputdir\n", "!wget \"https://www.dropbox.com/s/4g0pigmro4vo1b4/menutemplate?dl=0\" -O /tmp/tools/menutemplate >> /tmp/toollog 2>&1 \n", "!wget \"https://www.dropbox.com/s/3flttpzhsja8td7/construct_menu.py?dl=0\" -O /tmp/tools/construct_menu.py >> /tmp/toollog 2>&1 \n", "!python /tmp/tools/construct_menu.py \"{THE_NOTEBOOK}.ipynb\" {outputdir}\n", "from IPython.core.display import HTML\n", "output_file_name = outputdir + THE_NOTEBOOK.replace(\" \", \"\").replace(\"[\", \"\").replace(\"]\", \"\") + \".ipynb.html\"\n", "with open(output_file_name) as fp:\n", "    html = fp.read()\n", "HTML(html)"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["# Goals of the Laboratory\n", "In this introductory laboratory, we expect students to:\n", "\n", "1. Acquire basic knowledge about Python and Matplotlib\n", "2. Gain familiarity with Juypter Notebooks\n", "3. Gain familiarity with the PySpark API and SparkSQL\n", "\n", "To achieve such goals, we will go through the following steps:\n", "\n", "1. In section 1, **IPython** and **Jupyter Notebooks** are introduced to help students understand the environment used to work on Data Science projects.\n", "\n", "2. In section 2, we briefly overview **Python** and its syntax. In addition, we cover **Matplotlib**, a very powerful library to plot figures in Python, which you can use for your Data Science projects. Finally, we introduce **Pandas**, a python library that is very helpful when working on Data Science projects.\n", "\n", "3. In section 3 we cover the **PySpark** and **SparkSQL** APIs\n", "\n", "4. In section 4, we conclude the introductory laboratory with a simple use case.\n"], "cell_type": "markdown", "metadata": {}}, {"source": ["# 1. Python, IPython and Jupyter Notebooks\n", "\n", "**Python** is a high-level, dynamic, object-oriented programming language. It is a general purpose language, which is designed to be easy to use and easy to read.\n", "\n", "**IPython** (Interactive Python) is orignally developed for Python. Now, it is a command shell for interactive computing supporting multiple programming languages. It offers rich media, shell syntax, tab completion, and history. IPython is based on an architecture that provides parallel and distributed computing. IPython enables parallel applications to be developed, executed, debugged and monitored interactively.\n", "\n", "**Jupyter Notebooks** are a web-based interactive computational environment for creating IPython notebooks. An IPython notebook is a JSON document containing an ordered list of input/output cells which can contain code, text, mathematics, plots and rich media. Notebooks make data analysis easier to perform, understand and reproduce. All laboratories in this course are prepared as Notebooks. As you can see, in this Notebook, we can put text, images, hyperlinks, source code... The Notebooks can be converted to a number of open standard output formats (HTML, HTML presentation slides, LaTeX, PDF, ReStructuredText, Markdown, Python) through `File` -> `Download As` in the web interface. In addition, Jupyter manages the notebooks' versions through a `checkpoint` mechanism. You can create checkpoint anytime via `File -> Save and Checkpoint`. \n", "\n", "**NOTE on Checkpointing:** in this course, we use a peculiar environment to work. We don't have a Notebook server: instead, we creat on demand clusters with a Notebook front-end. Since your clusters are **emphemeral** (they are terminated after a pre-defined amount of time), checkpointing is of little use, for anything else than saving your notebook in your emphemeral environment. It is far better to download regularly your notebooks, and to push them to your git repository."], "cell_type": "markdown", "metadata": {}}, {"source": ["## 1.1. Tab completion\n", "\n", "Tab completion is a convenient way to explore the structure of any object you're dealing with. Simply type object_name.<TAB> to view the suggestion for object's attributes. Besides Python objects and keywords, tab completion also works on file and directory names."], "cell_type": "markdown", "metadata": {}}, {"execution_count": 3, "cell_type": "code", "source": ["s = \"test function of tab completion\"\n", "\n", "# type s.<TAB> to see the suggestions\n", "\n", "# Show your experiments working on a string. \n", "# Try splitting a string into its constituent words, and count the number of words.\n"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["## 1.2. System shell commands\n", "\n", "To run any command in the system shell, simply prefix it with `!`. For example:"], "cell_type": "markdown", "metadata": {}}, {"execution_count": 4, "cell_type": "code", "source": ["# list all file and directories in the current folder\n", "!ls"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["## 1.3. Magic functions\n", "\n", "IPython has a set of predefined `magic functions` that you can call with a command line style syntax. There are two types of magics, line-oriented and cell-oriented. \n", "\n", "**Line magics** are prefixed with the `%` character and work much like OS command-line calls: they get as an argument the rest of the line, *where arguments are passed without parentheses or quotes*. \n", "\n", "**Cell magics** are prefixed with a double `%%`, and they are functions that get as an argument not only the rest of the line, but also the lines below it in a separate argument."], "cell_type": "markdown", "metadata": {}}, {"execution_count": 5, "cell_type": "code", "source": ["%timeit range(1000)"], "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 6, "cell_type": "code", "source": ["%%timeit x = range(10000)\n", "max(x)"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["For more information, you can follow this [link](http://nbviewer.jupyter.org/github/ipython/ipython/blob/1.x/examples/notebooks/Cell%20Magics.ipynb)"], "cell_type": "markdown", "metadata": {}}, {"source": ["## 1.4. Debugging\n", "\n", "Whenever an exception occurs, the call stack is printed out to help you to track down the true source of the problem. It is important to gain familiarity with the call stack, especially when using the PySpark API."], "cell_type": "markdown", "metadata": {}}, {"execution_count": 7, "cell_type": "code", "source": ["for i in [4,3,2,0]:\n", "    print(5/i)"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["## 1.5. Additional features\n", "\n", "Jupyter also supports viewing the status of the cluster and interact with the real shell environment.\n", "\n", "To do that, you can click on the Logo Jupyter in the up-left coner of each notebook to go to the dashboard:\n", "\n", "<img src=\"https://farm2.staticflickr.com/1488/24681339931_733acb3494_b.jpg\" width=\"600px\" />\n", "\n", "You can easily find out how to use these features, so you're invited to play around!!"], "cell_type": "markdown", "metadata": {}}, {"source": ["# 2. Python + Pandas + Matplotlib: A great environment for Data Science\n", "\n", "This section aims to help students gain a basic understanding of the python programming language and some of its libraries, including `Pandas` or `Matplotlib`. \n", "\n", "When working with a small dataset (one that can comfortably fit into a single machine), Pandas and Matplotlib, together with Python are valid alternatives to other popular tools such as R and Matlab. Using such libraries allows to inherit from the simple and clear Python syntax, achieve very good performance, enjoy superior memory management,  error handling, and good package management \\[[1](http://ajminich.com/2013/06/22/9-reasons-to-switch-from-matlab-to-python/)\\].\n", "\n", "\n", "## 2.1. Python syntax\n", "\n", "(This section is for students who did not program in Python before. If you're familiar with Python, please move to the next section: 1.2. Numpy)\n", "\n", "When working with Python, the code seems to be simpler than (many) other languages. In this laboratory, we compare the Python syntax to that of Java - another very common language.\n", "\n", "```java\n", "// java syntax\n", "int i = 10;\n", "string s = \"advanced machine learning\";\n", "System.out.println(i);\n", "System.out.println(s);\n", "// you must not forget the semicolon at the end of each sentence\n", "```"], "cell_type": "markdown", "metadata": {}}, {"execution_count": 8, "cell_type": "code", "source": ["# python syntax\n", "i = 10\n", "s = \"advanced machine learning\"\n", "print(i)\n", "print(s)\n", "# forget about the obligation of commas"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["### Indentation & If-else syntax\n", "In python, we don't use `{` and `}` to define blocks of codes: instead, we use indentation to do that. **The code within the same block must have the same indentation**. For example, in java, we write:\n", "```java\n", "string language = \"Python\";\n", "\n", "// the block is surrounded by { and }\n", "// the condition is in ( and )\n", "if (language == \"Python\") {\n", "    int x = 1;\n", "    x += 10;\n", "       int y = 5; // a wrong indentation isn't problem\n", "    y = x + y;\n", "    System.out.println(x + y);\n", "    \n", "    // a statement is broken into two line\n", "    x = y\n", "        + y;\n", "    \n", "    // do some stuffs\n", "}\n", "else if (language == \"Java\") {\n", "    // another block\n", "}\n", "else {\n", "    // another block\n", "}\n", "```"], "cell_type": "markdown", "metadata": {}}, {"execution_count": 9, "cell_type": "code", "source": ["language = \"Python\"\n", "if language == \"Python\":\n", "    x = 10\n", "    x += 10\n", "    y = 5 # all statements in the same block must have the same indentation\n", "    y = (\n", "        x + y\n", "    ) # statements can be on multiple lines, using ( )\n", "    print (x \n", "           + y)\n", "    \n", "    # statements can also be split on multiple lines by using \\ at the END of each line\n", "    x = y \\\n", "        + y\n", "    \n", "    # do some other stuffs\n", "elif language == \"Java\":\n", "    # another block\n", "    pass\n", "else:\n", "    # another block\n", "    pass"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["### Ternary conditional operator\n", "In python, we often see ternary conditional operator, which is used to assign a value to a variable based on some condition. For example, in java, we write:\n", "\n", "```java\n", "int x = 10;\n", "// if x > 10, assign y = 5, otherwise, y = 15\n", "int y = (x > 10) ? 5 : 15;\n", "\n", "int z;\n", "if (x > 10)\n", "    z = 5; // it's not necessary to have { } when the block has only one statement\n", "else\n", "    z = 15;\n", "```\n", "\n", "Of course, although we can easily write these lines of code in an `if else` block to get the same result, people prefer ternary conditioinal operator because of simplicity.\n", "\n", "In python, we write:"], "cell_type": "markdown", "metadata": {}}, {"execution_count": 10, "cell_type": "code", "source": ["x = 10\n", "# a very natural way\n", "y = 5 if x > 10 else 15\n", "print(y)\n", "\n", "# another way\n", "y = x > 10 and 5 or 15\n", "print(y)"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["### Lists and For loops\n", "Another syntax that we should revisit is the `for loop`. In java, we can write:\n", "\n", "```java\n", "// init an array with 10 integer numbers\n", "int[] array = new int[]{1, 2, 3, 4, 5, 6, 7, 8, 9, 10};\n", "for (int i = 0; i < array.length; i++){\n", "    // print the i-th element of array\n", "    System.out.println(array[i]);\n", "}\n", "```\n", "\n", "In Python, instead of using an index to help indicating an element, we can access the element directly:"], "cell_type": "markdown", "metadata": {}}, {"execution_count": 11, "cell_type": "code", "source": ["array = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n", "# Python has no built-in array data structure\n", "# instead, it uses \"list\" which is much more general \n", "# and can be used as a multidimensional array quite easily.\n", "for element in array:\n", "    print(element)"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["As we can see, the code is very clean. If you need the index of each element, here's what you should do:"], "cell_type": "markdown", "metadata": {}}, {"execution_count": 12, "cell_type": "code", "source": ["for (index, element) in enumerate(array):\n", "    print(index, element)"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["Actually, Python has no built-in array data structure. It uses the `list` data structure, which is much more general and can be used as a multidimensional array quite easily. In addtion, elements in a list can be retrieved in a very concise way. For example, we create a 2d-array with 4 rows. Each row has 3 elements."], "cell_type": "markdown", "metadata": {}}, {"execution_count": 13, "cell_type": "code", "source": ["# 2-dimentions array with 4 rows, 3 columns\n", "twod_array = [[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]]\n", "for index, row in enumerate(twod_array):\n", "    print(\"row \", index, \":\", row)\n", "\n", "# print row 1 until row 3\n", "print(\"row 1 until row 3: \", twod_array[1:3])\n", "\n", "# all rows from row 2\n", "print(\"all rows from row 2: \", twod_array[2:])\n", "\n", "# all rows until row 2\n", "print(\"all rows until row 2:\", twod_array[:2])\n", "\n", "# all rows from the beginning with step of 2. \n", "print(\"all rows from the beginning with step of 2:\", twod_array[::2])"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["### Dictionaries\n", "Another useful data structure in Python is a `dictionary`, which we use to store (key, value) pairs. Here's some example usage of dictionaries:"], "cell_type": "markdown", "metadata": {}}, {"execution_count": 14, "cell_type": "code", "source": ["d = {'key1': 'value1', 'key2': 'value2'}  # Create a new dictionary with some data\n", "print(d['key1'])       # Get an entry from a dictionary; prints \"value1\"\n", "print('key1' in d)     # Check if a dictionary has a given key; prints \"True\"\n", "d['key3'] = 'value3'    # Set an entry in a dictionary\n", "print(d['key3'])      # Prints \"value3\"\n", "# print(d['key9'])  # KeyError: 'key9' not a key of d\n", "print(d.get('key9', 'custom_default_value'))  # Get an element with a default; prints \"custom_default_value\"\n", "print(d.get('key3', 'custom_default_value'))    # Get an element with a default; prints \"value3\"\n", "del d['key3']        # Remove an element from a dictionary\n", "print(d.get('key3', 'custom_default_value')) # \"fish\" is no longer a key; prints \"custom_default_value\"\n"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["### Functions\n", "In Python, we can define a function by using keyword `def`."], "cell_type": "markdown", "metadata": {}}, {"execution_count": 15, "cell_type": "code", "source": ["def square(x):\n", "    return x*x\n", "\n", "print(square(5))"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["You can apply a function to each element of a list/array by using `lambda` function. For example, we want to square elements in a list:"], "cell_type": "markdown", "metadata": {}}, {"execution_count": 16, "cell_type": "code", "source": ["array = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n", "\n", "# apply function \"square\" on each element of \"array\"\n", "print(list(map(lambda x: square(x), array)))\n", "\n", "# or using a for loop, and a list comprehension\n", "print([square(x) for x in array])\n", "\n", "print(\"orignal array:\", array)"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["These two above syntaxes are used very often. \n", "\n", "If you are not familiar with **list comprehensions**, follow this [link](http://python-3-patterns-idioms-test.readthedocs.io/en/latest/Comprehensions.html]).\n", "\n", "We can also put a function `B` inside a function `A` (that is, we can have nested functions). In that case, function `B` is only accessed inside function `A` (the scope that it's declared). For example:"], "cell_type": "markdown", "metadata": {}}, {"execution_count": 17, "cell_type": "code", "source": ["# select only the prime number in array\n", "# and square them\n", "def filterAndSquarePrime(arr):\n", "    \n", "    # a very simple function to check a number is prime or not\n", "    def checkPrime(number):\n", "        for i in range(2, int(number/2)):\n", "            if number % i == 0:\n", "                return False\n", "        return True\n", "    \n", "    primeNumbers = filter(lambda x: checkPrime(x), arr)\n", "    return map(lambda x: square(x), primeNumbers)\n", "\n", "# we can not access checkPrime from here\n", "# checkPrime(5)\n", "\n", "result = filterAndSquarePrime(array)\n", "list(result)"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["### Importing modules, functions\n", "Modules in Python are packages of code. Putting code into modules helps increasing the reusability and maintainability.\n", "The modules can be nested.\n", "To import a module, we simple use syntax: `import <module_name>`. Once it is imported, we can use any functions, classes inside it."], "cell_type": "markdown", "metadata": {}}, {"execution_count": 18, "cell_type": "code", "source": ["# import module 'math' to uses functions for calculating\n", "import math\n", "\n", "# print the square root of 16\n", "print(math.sqrt(16))\n", "\n", "# we can create alias when import a module\n", "import numpy as np\n", "\n", "print(np.sqrt(16))"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["Sometimes, you only need to import some functions inside a module to avoid loading the whole module into memory. To do that, we can use syntax: `from <module> import <function>`"], "cell_type": "markdown", "metadata": {}}, {"execution_count": 19, "cell_type": "code", "source": ["# only import function 'sin' in package 'math'\n", "from math import sin\n", "\n", "# use the function\n", "print(sin(60))"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["That's quite enough for Python. Now, let's practice a little bit.\n", "\n", "![](https://farm2.staticflickr.com/1604/24934700445_833f0a5649_t.jpg)"], "cell_type": "markdown", "metadata": {}}, {"source": ["<div class='anchor' ></div>\n", "\n", "### Question 1\n", "#### Question 1.1\n", "Write a function `checkSquareNumber` to check if a integer number is a square number or not. For example, 16 and 9 are square numbers. 15 isn't square number.\n", "Requirements:\n", "\n", "- Input: an integer number\n", "\n", "- Output: `True` or `False`\n", "\n", "HINT: If the quare root of a number is an integer number, it is a square number."], "cell_type": "markdown", "metadata": {}}, {"source": ["```python\n", "import math\n", "\n", "def checkSquareNumber(x):\n", "    # calculate the square root of x\n", "    # return True if square root is integer, \n", "    # otherwise, return False\n", "    return ...\n", "\n", "print(checkSquareNumber(16))\n", "print(checkSquareNumber(250))\n", "```"], "cell_type": "markdown", "metadata": {}}, {"source": ["<div class='anchor' ></div>\n", "#### Question 1.2\n", "A list `list_numbers` which contains the numbers from 1 to 9999 can be constructed from: \n", "\n", "```python\n", "list_numbers = range(0, 10000)\n", "```\n", "\n", "Extract the square numbers in `list_numbers` using function `checkSquareNumber` from question 1.1. How many elements in the extracted list ?"], "cell_type": "markdown", "metadata": {}}, {"source": ["```python\n", "list_numbers = ...\n", "square_numbers = # try to use the filter method\n", "print(square_numbers)\n", "print(len(square_numbers))\n", "```"], "cell_type": "markdown", "metadata": {}}, {"source": ["<div class='anchor' ></div>\n", "#### Question 1.3\n", "\n", "Using array slicing, select the elements of the list square_numbers, whose index is from 5 to 20 (zero-based index)."], "cell_type": "markdown", "metadata": {}}, {"source": ["```python\n", "print(square_numbers[...])\n", "```"], "cell_type": "markdown", "metadata": {}}, {"source": ["Next, we will take a quick look on Numpy - a powerful module of Python."], "cell_type": "markdown", "metadata": {}}, {"source": ["## 2.2. Numpy\n", "Numpy is the core library for scientific computing in Python. It provides a high-performance multidimensional array object, and tools for working with these arrays.\n", "### 2.2.1. Array\n", "A numpy array is a grid of values, all of **the same type**, and is indexed by a tuple of nonnegative integers. Thanks to the same type property, Numpy has the benefits of [locality of reference](https://en.wikipedia.org/wiki/Locality_of_reference). Besides, many other Numpy operations are implemented in C, avoiding the general cost of loops in Python, pointer indirection and per-element dynamic type checking. So, the speed of Numpy is often faster than using built-in datastructure of Python. When working with massive data with computationally expensive tasks, you should consider to use Numpy. \n", "\n", "The number of dimensions is the `rank` of the array; the `shape` of an array is a tuple of integers giving the size of the array along each dimension.\n", "\n", "We can initialize numpy arrays from nested Python lists, and access elements using square brackets:"], "cell_type": "markdown", "metadata": {}}, {"execution_count": 23, "cell_type": "code", "source": ["import numpy as np\n", "\n", "# Create a rank 1 array\n", "rank1_array = np.array([1, 2, 3])\n", "print(\"type of rank1_array:\", type(rank1_array))\n", "print(\"shape of rank1_array:\", rank1_array.shape)\n", "print(\"elements in rank1_array:\", rank1_array[0], rank1_array[1], rank1_array[2])\n", "\n", "# Create a rank 2 array\n", "rank2_array = np.array([[1,2,3],[4,5,6]])\n", "print(\"shape of rank2_array:\", rank2_array.shape)\n", "print(rank2_array[0, 0], rank2_array[0, 1], rank2_array[1, 0])"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["### 2.2.2. Array slicing\n", "Similar to Python lists, numpy arrays can be sliced. The different thing is that you must specify a slice for each dimension of the array because arrays may be multidimensional."], "cell_type": "markdown", "metadata": {}}, {"execution_count": 24, "cell_type": "code", "source": ["m_array = np.array([[1,2,3,4], [5,6,7,8], [9,10,11,12]])\n", "\n", "# Use slicing to pull out the subarray consisting of the first 2 rows\n", "# and columns 1 and 2\n", "b = m_array[:2, 1:3]\n", "print(b)\n", "\n", "# we can only use this syntax with numpy array, not python list\n", "print(\"value at row 0, column 1:\", m_array[0, 1])\n", "\n", "# Rank 1 view of the second row of m_array  \n", "print(\"the second row of m_array:\", m_array[1, :])\n", "\n", "# print element at position (0,2) and (1,3)\n", "print(m_array[[0,1], [2,3]])"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["### 2.2.3. Boolean array indexing\n", "We can use boolean array indexing to check whether each element in the array satisfies a condition or use it to do filtering."], "cell_type": "markdown", "metadata": {}}, {"execution_count": 25, "cell_type": "code", "source": ["m_array = np.array([[1,2,3,4], [5,6,7,8], [9,10,11,12]])\n", "\n", "# Find the elements of a that are bigger than 2\n", "# this returns a numpy array of Booleans of the same\n", "# shape as m_array, where each value of bool_idx tells\n", "# whether that element of a is > 3 or not\n", "bool_idx = (m_array > 3)\n", "print(bool_idx , \"\\n\")\n", "\n", "# We use boolean array indexing to construct a rank 1 array\n", "# consisting of the elements of a corresponding to the True values\n", "# of bool_idx\n", "print(m_array[bool_idx], \"\\n\")\n", "\n", "# We can combine two statements\n", "print(m_array[m_array > 3], \"\\n\")\n", "\n", "# select elements with multiple conditions\n", "print(m_array[(m_array > 3) & (m_array % 2 == 0)])\n"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["### 2.2.4. Datatypes\n", "Remember that the elements in a numpy array have the same type. When constructing arrays, Numpy tries to guess a datatype when you create an array However, we can specify the datatype explicitly via an optional argument."], "cell_type": "markdown", "metadata": {}}, {"execution_count": 26, "cell_type": "code", "source": ["# let Numpy guess the datatype\n", "x1 = np.array([1, 2])\n", "print(x1.dtype)\n", "\n", "# force the datatype be float64\n", "x2 = np.array([1, 2], dtype=np.float64)\n", "print(x2.dtype)"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["### 2.2.5. Array math\n", "Similar to Matlab or R, in Numpy, basic mathematical functions operate elementwise on arrays, and are available both as operator overloads and as functions in the numpy module."], "cell_type": "markdown", "metadata": {}}, {"execution_count": 27, "cell_type": "code", "source": ["x = np.array([[1,2],[3,4]], dtype=np.float64)\n", "y = np.array([[5,6],[7,8]], dtype=np.float64)\n", "# mathematical function is used as operator\n", "print(\"x + y =\", x + y, \"\\n\")\n", "\n", "# mathematical function is used as function\n", "print(\"np.add(x, y)=\", np.add(x, y), \"\\n\")\n", "\n", "# Unlike MATLAB, * is elementwise multiplication\n", "# not matrix multiplication\n", "print(\"x * y =\", x * y , \"\\n\")\n", "print(\"np.multiply(x, y)=\", np.multiply(x, y), \"\\n\")\n", "print(\"x*2=\", x*2, \"\\n\")\n", "\n", "# to multiply two matrices, we use dot function\n", "print(\"x.dot(y)=\", x.dot(y), \"\\n\")\n", "print(\"np.dot(x, y)=\", np.dot(x, y), \"\\n\")\n", "\n", "# Elementwise square root\n", "print(\"np.sqrt(x)=\", np.sqrt(x), \"\\n\")"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["Note that unlike MATLAB, `*` is elementwise multiplication, not matrix multiplication. We instead use the `dot` function to compute inner products of vectors, to multiply a vector by a matrix, and to multiply matrices. In what follows, we work on a few more examples to reiterate the concept."], "cell_type": "markdown", "metadata": {}}, {"execution_count": 28, "cell_type": "code", "source": ["# declare two vectors\n", "v = np.array([9,10])\n", "w = np.array([11, 12])\n", "\n", "# Inner product of vectors\n", "print(\"v.dot(w)=\", v.dot(w))\n", "print(\"np.dot(v, w)=\", np.dot(v, w))\n", "\n", "# Matrix / vector product\n", "print(\"x.dot(v)=\", x.dot(v))\n", "print(\"np.dot(x, v)=\", np.dot(x, v))\n", "\n", "# Matrix / matrix product\n", "print(\"x.dot(y)=\", x.dot(y))\n", "print(\"np.dot(x, y)=\", np.dot(x, y))"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["Additionally, we can do other aggregation computations on arrays such as `sum`, `nansum`, or `T`."], "cell_type": "markdown", "metadata": {}}, {"execution_count": 29, "cell_type": "code", "source": ["x = np.array([[1,2], [3,4]])\n", "\n", "# Compute sum of all elements\n", "print(np.sum(x))\n", "\n", "# Compute sum of each column\n", "print(np.sum(x, axis=0))\n", "\n", "# Compute sum of each row\n", "print(np.sum(x, axis=1))\n", "\n", "# transpose the matrix\n", "print(x.T)\n", "\n", "# Note that taking the transpose of a rank 1 array does nothing:\n", "v = np.array([1,2,3])\n", "print(v.T)  # Prints \"[1 2 3]\""], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["![](https://farm2.staticflickr.com/1604/24934700445_833f0a5649_t.jpg)"], "cell_type": "markdown", "metadata": {}}, {"source": ["<div class='anchor' ></div>\n", "\n", "### Question 2\n", "\n", "Given a 2D array:\n", "\n", "```\n", " 1  2  3  4\n", " 5  6  7  8 \n", " 9 10 11 12\n", "13 14 15 16\n", "```\n", "\n", "#### Question 2.1\n", "\n", "Print the all odd numbers in this array using `Boolean array indexing`."], "cell_type": "markdown", "metadata": {}}, {"source": ["```python\n", "array_numbers = np.array([\n", "        [1, 2, 3, 4],\n", "        [5, 6, 7, 8],\n", "        [9, 10, 11, 12],\n", "        [13, 14, 15, 16]\n", "    ])\n", "\n", "print(...)\n", "```"], "cell_type": "markdown", "metadata": {}}, {"source": ["<div class='anchor' ></div>\n", "#### Question 2.2\n", "\n", "Extract the second row and the third column in this array using `array slicing`."], "cell_type": "markdown", "metadata": {}}, {"source": ["```python\n", "print(array_numbers[...])\n", "print(array_numbers[...])\n", "```"], "cell_type": "markdown", "metadata": {}}, {"source": ["<div class='anchor' ></div>\n", "#### Question 2.3\n", "Calculate the sum of diagonal elements."], "cell_type": "markdown", "metadata": {}}, {"source": ["```python\n", "sum = 0\n", "for i in range(0, ...):\n", "    sum += array_numbers...\n", "    \n", "print(sum)\n", "```"], "cell_type": "markdown", "metadata": {}}, {"source": ["<div class='anchor' ></div>\n", "#### Question 2.4\n", "Print elementwise multiplication of the first row and the last row using numpy's functions.\n", "\n", "Print the inner product of these two rows.\n"], "cell_type": "markdown", "metadata": {}}, {"source": ["```python\n", "print(...)\n", "print(...)\n", "```"], "cell_type": "markdown", "metadata": {}}, {"source": ["## 2.3. Matplotlib\n", "\n", "As its name indicates, Matplotlib is a plotting library. It provides both a very quick way to visualize data from Python and publication-quality figures in many formats. The most important function in matplotlib is `plot`, which allows you to plot 2D data."], "cell_type": "markdown", "metadata": {}}, {"execution_count": 34, "cell_type": "code", "source": ["%matplotlib inline\n", "import matplotlib.pyplot as plt\n", "plt.plot([1,2,3,4])\n", "plt.ylabel('custom y label')\n", "plt.show()"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["In this case, we provide a single list or array to the `plot()` command, matplotlib assumes it is a sequence of y values, and automatically generates the x values for us. Since python ranges start with 0, the default x vector has the same length as y but starts with 0. Hence the x data are [0,1,2,3].\n", "\n", "In the next example, we plot figure with both x and y data. Besides, we want to draw dashed lines instead of the solid in default."], "cell_type": "markdown", "metadata": {}}, {"execution_count": 35, "cell_type": "code", "source": ["plt.plot([1, 2, 3, 4], [1, 4, 9, 16], 'r--')\n", "plt.show()\n", "\n", "plt.bar([1, 2, 3, 4], [1, 4, 9, 16], align='center')\n", "# labels of each column bar\n", "x_labels = [\"Type 1\", \"Type 2\", \"Type 3\", \"Type 4\"]\n", "# assign labels to the plot\n", "plt.xticks([1, 2, 3, 4], x_labels)\n", "\n", "plt.show()"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["If we want to merge two figures into a single one, subplot is the best way to do that. For example, we want to put two figures in a stack vertically, we should define a grid of plots with 2 rows and 1 column. Then, in each row, a single figure is plotted."], "cell_type": "markdown", "metadata": {}}, {"execution_count": 36, "cell_type": "code", "source": ["# Set up a subplot grid that has height 2 and width 1,\n", "# and set the first such subplot as active.\n", "plt.subplot(2, 1, 1)\n", "plt.plot([1, 2, 3, 4], [1, 4, 9, 16], 'r--')\n", "\n", "# Set the second subplot as active, and make the second plot.\n", "plt.subplot(2, 1, 2)\n", "plt.bar([1, 2, 3, 4], [1, 4, 9, 16])\n", "\n", "plt.show()"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["For more examples, please visit the [homepage](http://matplotlib.org/1.5.1/examples/index.html) of Matplotlib."], "cell_type": "markdown", "metadata": {}}, {"source": ["![](https://farm2.staticflickr.com/1604/24934700445_833f0a5649_t.jpg)"], "cell_type": "markdown", "metadata": {}}, {"source": ["<div class='anchor' ></div>\n", "\n", "###  Question 3\n", "Given a list of numbers from 0 to 9999.\n", "\n", "#### Question 3.1\n", "Calculate the histogram of numbers divisible by 3, 7, 11 in the list respectively.\n", "\n", "( Or in other word, how many numbers divisible by 3, 7, 11 in the list respectively ?)"], "cell_type": "markdown", "metadata": {}}, {"source": ["```python\n", "arr = np.array(...)\n", "divisors = [3, 7, 11]\n", "histogram = list(...)\n", "print(histogram)\n", "```"], "cell_type": "markdown", "metadata": {}}, {"source": ["<div class='anchor' ></div>\n", "#### Question 3.2\n", "Plot the histogram in a line chart."], "cell_type": "markdown", "metadata": {}}, {"source": ["```python\n", "%matplotlib inline\n", "import matplotlib.pyplot as plt\n", "\n", "# simple line chart\n", "plt.plot(histogram)\n", "x_indexes = ...\n", "x_names = list(...)\n", "plt.xticks(x_indexes, x_names)\n", "plt.show()\n", "```"], "cell_type": "markdown", "metadata": {}}, {"source": ["<div class='anchor' ></div>\n", "#### Question 3.3\n", "Plot the histogram in a bar chart."], "cell_type": "markdown", "metadata": {}}, {"source": ["```python\n", "# char chart with x-lables\n", "x_indexes = range(...)\n", "x_names = list(...)\n", "plt.bar( x_indexes, histogram, align='center')\n", "plt.xticks(x_indexes, x_names)\n", "plt.show()\n", "```"], "cell_type": "markdown", "metadata": {}}, {"source": ["## 2.4. Pandas\n", "\n", "Pandas is an open source library providing high-performance, easy-to-use data structures and data analysis tools for the Python programming language. Indeeed, it is great for data manipulation, data analysis, and data visualization.\n", "\n", "### 2.4.1. Data structures\n", "Pandas introduces two useful (and powerful) structures: `Series` and `DataFrame`, both of which are built on top of NumPy.\n", "\n", "#### Series\n", "A `Series` is a one-dimensional object similar to an array, list, or even column in a table. It assigns a *labeled index* to each item in the Series. By default, each item will receive an index label from `0` to `N-1`, where `N` is the number items of `Series`.\n", "\n", "We can create a Series by passing a list of values, and let pandas create a default integer index.\n"], "cell_type": "markdown", "metadata": {}}, {"execution_count": 40, "cell_type": "code", "source": ["import pandas as pd\n", "import numpy as np\n", "\n", "# create a Series with an arbitrary list\n", "s = pd.Series([3, 'Machine learning', 1.414259, -65545, 'Happy coding!'])\n", "print(s)"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["Or, an index can be used explixitly when creating the `Series`."], "cell_type": "markdown", "metadata": {}}, {"execution_count": 41, "cell_type": "code", "source": ["s = pd.Series([3, 'Machine learning', 1.414259, -65545, 'Happy coding!'],\n", "             index=['Col1', 'Col2', 'Col3', 4.1, 5])\n", "print(s)"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["A `Series` can be constructed from a dictionary too."], "cell_type": "markdown", "metadata": {}}, {"execution_count": 42, "cell_type": "code", "source": ["s = pd.Series({\n", "        'Col1': 3, 'Col2': 'Machine learning', \n", "        'Col3': 1.414259, 4.1: -65545, \n", "        5: 'Happy coding!'\n", "    })\n", "print(s)"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["We can access items in a `Series` in a same way as `Numpy`."], "cell_type": "markdown", "metadata": {}}, {"execution_count": 43, "cell_type": "code", "source": ["s = pd.Series({\n", "        'Col1': 3, 'Col2': -10, \n", "        'Col3': 1.414259, \n", "        4.1: -65545, \n", "        5: 8\n", "    })\n", "\n", "# get element which has index='Col1'\n", "print(\"s['Col1']=\", s['Col1'], \"\\n\")\n", "\n", "# get elements whose index is in a given list\n", "print(\"s[['Col1', 'Col3', 4.5]]=\", s[['Col1', 'Col3', 4.5]], \"\\n\")\n", "\n", "# use boolean indexing for selection\n", "print(s[s > 0], \"\\n\")\n", "\n", "# modify elements on the fly using boolean indexing\n", "s[s > 0] = 15\n", "\n", "print(s, \"\\n\")\n", "\n", "# mathematical operations can be done using operators and functions.\n", "print(s*10,  \"\\n\")\n", "print(np.square(s), \"\\n\")"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["#### DataFrame\n", "A DataFrame is a tablular data structure comprised of rows and columns, akin to database table, or R's data.frame object. In a loose way, we can also think of a DataFrame as a group of Series objects that share an index (the column names).\n", "\n", "We can create a DataFrame by passing a dict of objects that can be converted to series-like."], "cell_type": "markdown", "metadata": {}}, {"execution_count": 44, "cell_type": "code", "source": ["data = {'year': [2013, 2014, 2015, 2013, 2014, 2015, 2013, 2014],\n", "        'team': ['Manchester United', 'Chelsea', 'Asernal', 'Liverpool', 'West Ham', 'Newcastle', 'Machester City', 'Tottenham'],\n", "        'wins': [11, 8, 10, 15, 11, 6, 10, 4],\n", "        'losses': [5, 8, 6, 1, 5, 10, 6, 12]}\n", "football = pd.DataFrame(data, columns=['year', 'team', 'wins', 'losses'])\n", "football"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["We can store data as a CSV file, or read data from a CSV file."], "cell_type": "markdown", "metadata": {}}, {"execution_count": 45, "cell_type": "code", "source": ["# save data to a csv file without the index\n", "football.to_csv('football.csv', index=False)\n", "\n", "from_csv = pd.read_csv('football.csv')\n", "from_csv.head()"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["To read a CSV file with a custom delimiter between values and custom columns' names, we can use parameters `sep` and `names` relatively.\n", "Moreover, Pandas also supports to read and write to [Excel file](http://pandas.pydata.org/pandas-docs/stable/io.html#io-excel) , sqlite database file, URL,  or even clipboard.\n", "\n", "We can have an overview on the data by using functions `info` and `describe`."], "cell_type": "markdown", "metadata": {}}, {"execution_count": 46, "cell_type": "code", "source": ["print(football.info(), \"\\n\")\n", "football.describe()"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["Numpy's regular slicing syntax works as well."], "cell_type": "markdown", "metadata": {}}, {"execution_count": 47, "cell_type": "code", "source": ["print(football[0:2], \"\\n\")\n", "\n", "# show only the teams that have won more than 10 matches from 2014\n", "print(football[(football.year >= 2014) & (football.wins >= 10)])"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["An important feature that Pandas supports is `JOIN`. Very often, the data comes from multiple sources, in multiple files. For example, we have 2 CSV files, one contains the information of Artists, the other contains information of Songs. If we want to query the artist name and his/her corresponding songs, we have to do joining two dataframe.\n", "\n", "Similar to SQL, in Pandas, you can do inner join, left outer join, right outer join and full outer join. Let's see a small example. Assume that we have two dataset of singers and songs. The relationship between two datasets is maintained by a constrain on `singer_code`."], "cell_type": "markdown", "metadata": {}}, {"execution_count": 48, "cell_type": "code", "source": ["singers = pd.DataFrame({'singer_code': range(5), \n", "                           'singer_name': ['singer_a', 'singer_b', 'singer_c', 'singer_d', 'singer_e']})\n", "songs = pd.DataFrame({'singer_code': [2, 2, 3, 4, 5], \n", "                           'song_name': ['song_f', 'song_g', 'song_h', 'song_i', 'song_j']})\n", "print(singers)\n", "print('\\n')\n", "print(songs)"], "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 49, "cell_type": "code", "source": ["# inner join\n", "pd.merge(singers, songs, on='singer_code', how='inner')"], "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 50, "cell_type": "code", "source": ["# left join\n", "pd.merge(singers, songs, on='singer_code', how='left')"], "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 51, "cell_type": "code", "source": ["# right join\n", "pd.merge(singers, songs, on='singer_code', how='right')"], "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 52, "cell_type": "code", "source": ["# outer join (full join)\n", "pd.merge(singers, songs, on='singer_code', how='outer')"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["We can also concat two dataframes vertically or horizontally via function `concat` and parameter `axis`. This function is useful when we need to append two similar datasets or to put them side by site"], "cell_type": "markdown", "metadata": {}}, {"execution_count": 53, "cell_type": "code", "source": ["# concat vertically\n", "pd.concat([singers, songs])"], "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 54, "cell_type": "code", "source": ["# concat horizontally\n", "pd.concat([singers, songs], axis=1)"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["When computing descriptive statistic, we usually need to aggregate data by each group. For example, to anwser the question \"how many songs each singer has?\", we have to group data by each singer, and then calculate the number of songs in each group. Not that the result must contain the statistic of all singers in database (even if some of them have no song)"], "cell_type": "markdown", "metadata": {}}, {"execution_count": 55, "cell_type": "code", "source": ["data = pd.merge(singers, songs, on='singer_code', how='left')\n", "\n", "# count the values of each column in group\n", "print(data.groupby('singer_code').count())\n", "\n", "print(\"\\n\")\n", "\n", "# count only song_name\n", "print(data.groupby('singer_code').song_name.count())\n", "\n", "print(\"\\n\")\n", "\n", "# count song name but ignore duplication, and order the result\n", "print(data.groupby('singer_code').song_name.nunique().sort_values(ascending=True))"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["![](https://farm2.staticflickr.com/1604/24934700445_833f0a5649_t.jpg)"], "cell_type": "markdown", "metadata": {}}, {"source": ["<div class='anchor' ></div>\n", "\n", "\n", "### Question 4\n", "\n", "We have two datasets about music: [song](https://github.com/michiard/AML-COURSE/blob/master/data/song.tsv) and [album](https://github.com/michiard/AML-COURSE/blob/master/data/album.tsv).\n", "\n", "In the following questions, you **have to** use Pandas to load data and write code to answer these questions.\n", "\n", "#### Question 4.1\n", "Load both dataset into two dataframes and print the information of each dataframe\n", "\n", "**HINT**: \n", "\n", "- You can click button `Raw` on the github page of each dataset and copy the URL of the raw file.\n", "- The dataset can be load by using function `read_table`. For example: `df = pd.read_table(raw_url, sep='\\t')`"], "cell_type": "markdown", "metadata": {}}, {"source": ["```python\n", "import pandas as pd\n", "\n", "songdb_url = 'https://raw.githubusercontent.com/DistributedSystemsGroup/Algorithmic-Machine-Learning/master/data/song.tsv'\n", "albumdb_url = 'https://raw.githubusercontent.com/DistributedSystemsGroup/Algorithmic-Machine-Learning/master/data/album.tsv'\n", "song_df = pd...\n", "album_df = pd...\n", "\n", "print(song_df...)\n", "print(album_df...)\n", "```"], "cell_type": "markdown", "metadata": {}}, {"source": ["<div class='anchor' ></div>\n", "\n", "#### Question 4.2\n", "How many albums in this datasets ?\n", "\n", "How many songs in this datasets ?"], "cell_type": "markdown", "metadata": {}}, {"source": ["```python\n", "print(\"number of albums:\", album_df....count())\n", "print(\"number of songs:\", song_df.Song...)\n", "```"], "cell_type": "markdown", "metadata": {}}, {"source": ["<div class='anchor' ></div>\n", "#### Question 4.3\n", "How many distinct singers in this dataset ?"], "cell_type": "markdown", "metadata": {}}, {"source": ["```python\n", "print(\"number distinct singers:\", len(...))\n", "```"], "cell_type": "markdown", "metadata": {}}, {"source": ["<div class='anchor' ></div>\n", "#### Question 4.4\n", "Is there any song that doesn't belong to any album ?\n", "\n", "Is there any album that has no song ?\n", "\n", "**HINT**: \n", "\n", "- To join two datasets on different key names, we use `left_on=` and `right_on=` instead of `on=`.\n", "- Funtion `notnull` and `isnull` help determining the value of a column is missing or not. For example:\n", "`df['song'].isnull()`."], "cell_type": "markdown", "metadata": {}}, {"source": ["```python\n", "fulldf = pd.merge(song_df, album_df, how='outer', left_on='Album', right_on='Album code')\n", "fulldf[fulldf['Song'].... & fulldf['Album']....]\n", "```"], "cell_type": "markdown", "metadata": {}}, {"source": ["```python\n", "fulldf[fulldf['Song'].... & fulldf['Album code']....]\n", "```"], "cell_type": "markdown", "metadata": {}}, {"source": ["<div class='anchor' ></div>\n", "\n", "#### Question 4.5\n", "How many songs in each albums of Michael Jackson ?"], "cell_type": "markdown", "metadata": {}}, {"source": ["```python\n", "\n", "# Try thinking like as for map reduce word count!!\n", "\n", "fulldf[fulldf['Singer']=='Michael Jackson']....\n", "```"], "cell_type": "markdown", "metadata": {}}, {"source": ["# 3. PySpark\n", "\n", "Spark is an alternative framework to Hadoop MapReduce, designed to make it easier and quicker to build and run distributed data manipulation algorithms. Spark comes with a library for machine learning (MLLib) and graph algorithms, and also supports real-time streaming and SQL syntax, via Spark Streaming and SparkSQL, respectively. Spark exposes the Spark programming model to Java, Scala, or Python. In Python, we use the PySpark API to interact with Spark.\n", "\n", "As discussed in the introductory lecture, every Spark application has a Spark driver. It is the program that declares the transformations and actions on RDDs of data and submits such requests to the cluster manager. Actually, the driver is the program that creates the `SparkContext`, connecting to a given cluster manager such as  Spark Master, YARN or others. The executors run user code, run computations and can cache data for your application. The `SparkContext` will create a job that is broken into stages. The stages are broken into tasks which are scheduled by the SparkContext on an executor.\n", "\n", "When starting PySpark with command `pyspark` or using a well configured notebook (such as this one), `SparkContext` is created automatically in variable `sc`. \n"], "cell_type": "markdown", "metadata": {}}, {"execution_count": 1, "cell_type": "code", "source": ["sc"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["PySpark uses PySpark RDDs which  are just RDDs of Python objects: like Python lists, they can store objects with mixed types (actually all the objects are instances of `PyObject`).\n", "\n", "When PySpark is started, it also starts a JVM, which is accessible through a socket. PySpark uses `Py4J` to handle this communication. The JVM works as the actual Spark driver, and loads a `JavaSparkContext` that communicates with the Spark executors across the cluster. Python API calls to the Spark Context object are then **translated into Java API calls** to the JavaSparkContext. For example, the implementation of PySpark's `sc.textFile()` dispatches a call to the `.textFile` method of the `JavaSparkContext`, which ultimately communicates with the Spark executor JVMs to load the text data from HDFS. \n", "\n", "![](http://i.imgur.com/YlI8AqEl.png)\n", "\n", "The Spark executors on the cluster start a Python interpreter for each core, with which they communicate data through a pipe when they need to execute user-code. A Python RDD in the local PySpark client corresponds to a `PythonRDD` object in the local JVM. The data associated with the RDD actually lives in the Spark JVMs as Java objects. For example, running `sc.textFile()` in the Python interpreter will call the `JavaSparkContexts` `textFile` method, which loads the data as Java String objects in the cluster.\n", "\n", "\n", "When an API call is made on the `PythonRDD`, any associated code (e.g., Python lambda function) **is serialized and distributed to the executors**. The data is then converted from Java objects to a Python-compatible representation (e.g., pickle objects) and streamed to executor-associated Python interpreters through a pipe. Any necessary Python processing is executed in the interpreter, and the resulting data is stored back as an RDD (as pickle objects by default) in the JVMs. \n"], "cell_type": "markdown", "metadata": {}}, {"source": ["The data is read easily by using functions of Spark Context. For example, to read a text file and count the number of lines, we can write:\n"], "cell_type": "markdown", "metadata": {}}, {"execution_count": 2, "cell_type": "code", "source": ["# each line is stored as an element in 'words' - a PythonRDD.\n", "words = sc.textFile(\"/datasets/gutenberg/gutenberg_tiny.txt\")\n", "num_lines = words.count()\n", "print(\"the number of lines in file\", num_lines)"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["## 3.1. Wordcount example\n", "In the example below, we are interested in the top-10 words in terms of frequency of occurrence. To do so, we use a small text file as an input, and we wish to plot the term frequency of such top-10 words using Matplotlib.\n", "\n", "First, using the method `textFile` from the SparkContext `sc`, we create a RDD of strings. Each string in the RDD is representative for a line in the text file. In a loose way, we can think the first RDD is a RDD of lines. \n", "\n", "Because we work on the scope of words, we have to transform **a line** of the current RDD into **multiple words**, each word is an object of the new RDD. This is done by using `flatMap` function. \n", "\n", "Then, a `map` function transforms **each word** in the RDD into **a single** tuple with 2 components: the word itselft and the count of 1. As you might have guessed, this is a PairRDD, where each object is a key-value pair. \n", "\n", "We can take advantage of function `reduceByKey` to sum all frequencies of the same word. Now, each element in the RDD is in the form of: (word, total_frequency). To sort the words by frequency of occurrrence, we can use many approaches. One of the simplest approach is swap each tuple such that the frequency becomes the key, and use the `sortByKey` function."], "cell_type": "markdown", "metadata": {}}, {"execution_count": 3, "cell_type": "code", "source": ["words = (\n", "            # read the text file\n", "            sc.textFile(\"/datasets/gutenberg/gutenberg_tiny.txt\")\n", "            \n", "            # construct words from lines\n", "            .flatMap(lambda line: line.split())\n", "            \n", "            # map each word to (word, 1)\n", "            .map(lambda x: (x, 1))\n", "    \n", "            # reduce by key: accumulate sum the freq of the same word\n", "            .reduceByKey(lambda freq1, freq2: freq1 + freq2)\n", "            \n", "            # swap (word, freq) to (freq, word)\n", "            .map(lambda x: (x[1], x[0]))\n", "    \n", "            # sort result by key DESC\n", "            .sortByKey(False)\n", "         )"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["Now the top-10 words are collected and sent back to the driver by using function `take`."], "cell_type": "markdown", "metadata": {}}, {"execution_count": 4, "cell_type": "code", "source": ["# top 10 words:\n", "top10 = words.take(10)\n", "print(top10)"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["The action `collect` gathers all elements of the RDD (that reside on multiple machines) to the driver (which is running in a single machine), and cast it as a list.\n", "\n", "**ATTENTION** collecting an RDD in the driver can be problematic: indeed, an RDD can be very big in size (this is why they are distributed across several machines in the first place!) and thus it could deplete the RAM available in the machine running the driver!"], "cell_type": "markdown", "metadata": {}}, {"execution_count": 5, "cell_type": "code", "source": ["# collect results from executors to the driver\n", "# results = words.collect()\n", "# If you want to have a look at the results, don't print them all, for otherwise the notebook size will be too big"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["Recall that there are two kinds of functions in Spark: **transformations** and **actions**. All functions `map`, `flatMap`, `reduceByKey`, `sortByKey` are transformation functions. They are not executed right away when called. Indeed, Spark is lazyily evaluated, so nothing gets executed unless the driver invokes actions such as `count`, `take`, `collect`...\n", "\n", "RDD transformations allow us to create dependencies between RDDs. Each RDD in the lineage chain (string of dependencies) has a function for calculating its data and has a pointer (dependency) to its parent RDD. Everytime we use an RDD, dependencies are computed again from the beginning, which can be costly. Fortunatly, we can use the function `cache` to instruct Spark to checkpoint in RAM (but eventually also on disk) a particular RDD."], "cell_type": "markdown", "metadata": {}}, {"source": ["Let's now use the previous results from the execution of our simple Spark word count job, to plot word frequency information."], "cell_type": "markdown", "metadata": {}}, {"execution_count": 6, "cell_type": "code", "source": ["%matplotlib inline\n", "import matplotlib.pyplot as plt\n", "\n", "# extract the frequencies from the result\n", "frequencies = [x[0] for x in top10]\n", "\n", "# plot the frequencies\n", "plt.plot(frequencies)\n", "plt.show()"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["## 3.2. Night flights example\n", "We have a CSV file which contains the information about flights that took place in the US in 1994.\n", "The data in this file has 29 columns such as `year`, `month`, `day_of_month`, `scheduled_departure_time`,...\n", "We can have a quick look on the data:"], "cell_type": "markdown", "metadata": {}}, {"execution_count": 7, "cell_type": "code", "source": ["! hdfs dfs -cat /datasets/airline/1994.csv | head -n 10"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["In this simple example, we are interested only in columns: `CRSDepTime` (scheduled departure time) and `UniqueCarrier` (carrier identifier). The values of `CRSDepTime` are expressed in the format: hhmm (hour-minute).\n", "Assume that a flight is considered as a 'night flight' if its scheduled departured time `CRSDepTime` is later than 18:00.\n", "\n", "Questions:\n", "\n", "- How many night flights do we have in our data ?\n", "- How many night flights per unique carrier ? Plot the top-5 of them, in terms of volume\n", "\n", "First, we read the data and remove the header. Then, from the lines, we extract the information of scheduled departure time and carrier."], "cell_type": "markdown", "metadata": {}}, {"execution_count": 8, "cell_type": "code", "source": ["# read the data\n", "data = sc.textFile('/datasets/airline/1994.csv')\n", "\n", "# extract information about scheduled departure time and carrier\n", "# note that the scheduled time must be convert from string to interger number\n", "def extract_CRSDepTime_Carier(line):\n", "    cols = line.split(\",\")\n", "    return (int(cols[5]), cols[8])\n", "\n", "header = data.first()\n", "\n", "# remove header\n", "data_without_header = data.filter(lambda line: line != header)\n", "\n", "# create a new RDD with only scheduled departure time and carrier information\n", "# cache it for later usage\n", "newdata = (\n", "            data_without_header\n", "               .map(extract_CRSDepTime_Carier)\n", "               .cache()\n", "          )\n"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["Function `filter` helps us select only the objects that satisfy a condition. In this case, it creates a new RDD by filtering out the header. We can also use it to select the night flights."], "cell_type": "markdown", "metadata": {}}, {"execution_count": 9, "cell_type": "code", "source": ["night_flights = newdata.filter(lambda f: f[0] > 1800).cache()\n", "night_flights.take(3)"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["We use `cache` because we dont want to recalculate `night_flights` from the beginning everytime of using it."], "cell_type": "markdown", "metadata": {}}, {"execution_count": 10, "cell_type": "code", "source": ["# filter and count the night flights\n", "num_night_flights = night_flights.count()\n", "print(num_night_flights)"], "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 11, "cell_type": "code", "source": ["# group by carrier\n", "night_flights_by_carrier = night_flights.groupBy(lambda x: x[1]).mapValues(lambda flights: len(flights))\n", "\n", "# take top 5 carriers\n", "top5_carriers = night_flights_by_carrier.takeOrdered(5, key=lambda x: -x[1])\n", "\n", "print(top5_carriers)"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["We use `groupBy` to cluster flights which belong to the same carrier into a group. In this example, to select the top-5 carriers, we use a method based on the function `takeOrder`. This function takes the top-`k` objects ordered by an index: the trick is that we instruct it to use the cumulative counts as the key.\n", "\n", "Let's plot a bar chart using Matplotlib. To draw a bar chart, we use function `bar` which requires two parameters. Each parameter is a list of float values in each dimension."], "cell_type": "markdown", "metadata": {}}, {"execution_count": 12, "cell_type": "code", "source": ["%matplotlib inline\n", "import matplotlib.pyplot as plt\n", "\n", "# extract the number of flights which will be used as y-values\n", "num_flights = [ x[1] for x in top5_carriers]\n", "\n", "# extract the carriers' names\n", "carrier_names = [x[0] for x in top5_carriers]\n", "\n", "# create `virtual indexes for carriers which will be used as x-values`\n", "carrier_indexes = range(0, len(carrier_names))\n", "\n", "# plot\n", "plt.bar(carrier_indexes, num_flights, align=\"center\")\n", "\n", "# put x-labels for the plot\n", "plt.xticks(carrier_indexes, carrier_names)\n", "plt.show()"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["![](https://farm2.staticflickr.com/1604/24934700445_833f0a5649_t.jpg)"], "cell_type": "markdown", "metadata": {}}, {"source": ["<div class='anchor' ></div>\n", "\n", "\n", "## Question 5\n", "\n", "\n", "### Question 5.1\n", "Compute how many flights have a scheduled departure time later than 09:00 and before 14:00."], "cell_type": "markdown", "metadata": {}}, {"source": ["```python\n", "# read the data\n", "data = sc.textFile('/datasets/airline/1994.csv')\n", "\n", "# extract information about scheduled departure time and carrier\n", "# note that the scheduled time must be convert from string to interger number\n", "def extract_CRSDepTime_Carier(line):\n", "    ...\n", "    ...\n", "    return (int(cols[5]), cols[16])\n", "\n", "header = data.first()\n", "\n", "# remove header\n", "data_without_header = data.filter(...)\n", "\n", "# create RDD with only scheduled departure time and carrier information\n", "# cache it for later usages\n", "newdata = (\n", "            data_without_header\n", "               .map(extract_CRSDepTime_Carier)\n", "               ...\n", "          )\n", "\n", "flights = newdata.filter(...).cache()\n", "\n", "print(flights.count())\n", "```"], "cell_type": "markdown", "metadata": {}}, {"source": ["<div class='anchor' ></div>\n", "### Question 5.2\n", "Compute the cumulative number flights that have a scheduled departure time after 09:00 and before 14:00, for each source airport (origin). Plot the top-5 of such airports."], "cell_type": "markdown", "metadata": {}}, {"source": ["```python\n", "flights_per_carrier = flights.groupBy(...).mapValues(...)\n", "\n", "# take top 5 source airports\n", "top5_source_airport = flights_per_carrier.takeOrdered(...)\n", "\n", "%matplotlib inline\n", "import matplotlib.pyplot as plt\n", "\n", "# extract the number of flights which will be used as y-values\n", "# This is called list comprehension\n", "num_flights = [ x[1] for x in top5_source_airport]\n", "\n", "# create `virtual indexes for carriers which will be used as x-values`\n", "airport_indexes = range(0, len(top5_source_airport))\n", "\n", "# plot\n", "plt.bar(airport_indexes, num_flights, align=\"center\")\n", "\n", "# extract the carriers' names\n", "airport_names = [ x[0] for x in top5_source_airport]\n", "\n", "# put x-labels for the plot\n", "plt.xticks(airport_indexes, airport_names)\n", "plt.show()\n", "```"], "cell_type": "markdown", "metadata": {}}, {"source": ["# 4. Spark SQL and DataFrames\n", "\n", "One of the main features we suggest to use when analyzing data with Spark is `Spark SQL` - a module for structured data processing. Unlike the basic Spark RDD API, the interfaces provided by Spark SQL enriches Spark with more information about the structure of both the data and the computation being performed. Internally, this extra information is used to perform extra optimizations. There are several ways to interact with Spark SQL including SQL, the DataFrames API and the Datasets API. In this course, we mainly focus on `DataFrame API`. \n", "\n", "A `DataFrame` is a distributed collection of data organized into named columns. It is based on the data frame concept in R language or in Pandas. So, it is similar to a database table in a relational database.\n", "\n", "`DataFrames` can be constructed from a wide array of sources such as: structured data files, tables in Hive, external databases, or existing RDDs.\n", "\n", "During the labs in this course, we will mainly work with CSV data files. So, in the next sections, we only focus on constructing dataframes from structured data files directly and from existing RDD.\n", "\n", "## 4.1. Constructing a DataFrame directly from structured data file\n", "\n", "To construct DataFrame from a structured file directly, the file type must be supported. Currently, Spark supports  csv, json, avro and many more.\n", "Among these types, the csv type is one of the most popular in data analytic. A DataFrame is constructed from csv files by using the package `spark-csv` from Databrick."], "cell_type": "markdown", "metadata": {}}, {"execution_count": 15, "cell_type": "code", "source": ["from pyspark.sql import SQLContext\n", "from pyspark.sql.types import *\n", "\n", "sqlContext = SQLContext(sc)\n", "\n", "df = sqlContext.read.load('/datasets/airline/1994.csv', \n", "                          format='com.databricks.spark.csv', \n", "                          header='true', \n", "                          inferSchema='true',\n", "                          nullValue='NA'\n", "                        )"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["Using the function `load` parametrized with `com.databricks.spark.csv`, we ask the SqlContext to use the parser from DataBrick's package. Additionally, we can specify whether the file has a header, or ask the parser to guess the data type of columns automatically. The parsed data types is viewed using the function `printSchema`."], "cell_type": "markdown", "metadata": {}}, {"execution_count": 16, "cell_type": "code", "source": ["# print(df.dtypes)\n", "df.printSchema()"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["As you can see, the automatically inferred data types are not as expected. For example, we expect that `CRSDepTime` to be of interger type. The type and the name of each column can be modified using function `withColumn` and `withColumnRename` respectively. Additionally, we can also compute and print basic descriptive statistics of numerical columns via function `describe` (similar to Pandas)."], "cell_type": "markdown", "metadata": {}}, {"execution_count": 17, "cell_type": "code", "source": ["\n", "df = (df\n", "          # change type of column CRSDepTime by casting its values to interger type\n", "          .withColumn('CRSDepTime', df.CRSDepTime.cast('int'))\n", "      \n", "          # rename the column\n", "          .withColumnRenamed('CRSDepTime', 'scheduled_departure_time')\n", "    )\n", "\n", "# print schema of the current data\n", "df.printSchema()\n", "\n", "# run jobs to calculate basic statistic information and show it\n", "df.describe().show()"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["## 4.2. Constructing a DataFrame from an existing RDD\n", "Another way to construct a DataFrame is using data from an existing RDD. The main advantage of this approach is that it does not need a third party library. However, with this method, we have to remove the header ourself and provide a clear schema. "], "cell_type": "markdown", "metadata": {}}, {"execution_count": 18, "cell_type": "code", "source": ["from pyspark.sql import SQLContext\n", "from pyspark.sql.types import *\n", "\n", "sqlContext = SQLContext(sc)\n", "\n", "data = sc.textFile('/datasets/airline/1994.csv')\n", "\n", "# extract the header\n", "header = data.first()\n", "\n", "# replace invalid data with NULL and remove header\n", "cleaned_data = (\n", "        data\n", "    \n", "        # filter out the header\n", "        .filter(lambda line: line != header)\n", "    \n", "         # remove the 'missing data' by empty value\n", "        .map(lambda l: l.replace(',NA', ','))\n", "    )\n", "\n", "airline_data_schema = StructType([ \\\n", "    #StructField( name, dataType, nullable)\n", "    StructField(\"year\",                     IntegerType(), True), \\\n", "    StructField(\"month\",                    IntegerType(), True), \\\n", "    StructField(\"day_of_month\",             IntegerType(), True), \\\n", "    StructField(\"day_of_week\",              IntegerType(), True), \\\n", "    StructField(\"departure_time\",           IntegerType(), True), \\\n", "    StructField(\"scheduled_departure_time\", IntegerType(), True), \\\n", "    StructField(\"arrival_time\",             IntegerType(), True), \\\n", "    StructField(\"scheduled_arrival_time\",   IntegerType(), True), \\\n", "    StructField(\"carrier\",                  StringType(),  True), \\\n", "    StructField(\"flight_number\",            StringType(),  True), \\\n", "    StructField(\"tail_number\",              StringType(), True), \\\n", "    StructField(\"actual_elapsed_time\",      IntegerType(), True), \\\n", "    StructField(\"scheduled_elapsed_time\",   IntegerType(), True), \\\n", "    StructField(\"air_time\",                 IntegerType(), True), \\\n", "    StructField(\"arrival_delay\",            IntegerType(), True), \\\n", "    StructField(\"departure_delay\",          IntegerType(), True), \\\n", "    StructField(\"src_airport\",              StringType(),  True), \\\n", "    StructField(\"dest_airport\",             StringType(),  True), \\\n", "    StructField(\"distance\",                 IntegerType(), True), \\\n", "    StructField(\"taxi_in_time\",             IntegerType(), True), \\\n", "    StructField(\"taxi_out_time\",            IntegerType(), True), \\\n", "    StructField(\"cancelled\",                StringType(),  True), \\\n", "    StructField(\"cancellation_code\",        StringType(),  True), \\\n", "    StructField(\"diverted\",                 StringType(),  True), \\\n", "    StructField(\"carrier_delay\",            IntegerType(), True), \\\n", "    StructField(\"weather_delay\",            IntegerType(), True), \\\n", "    StructField(\"nas_delay\",                IntegerType(), True), \\\n", "    StructField(\"security_delay\",           IntegerType(), True), \\\n", "    StructField(\"late_aircraft_delay\",      IntegerType(), True)\\\n", "])"], "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 19, "cell_type": "code", "source": ["# convert each line into a tuple of features (columns) with the corresponding data type\n", "cleaned_data_to_columns = (\n", "    cleaned_data.map(lambda l: l.split(\",\"))\n", "    .map(lambda cols: \n", "         (\n", "            int(cols[0])  if cols[0] else None,\n", "            int(cols[1])  if cols[1] else None,\n", "            int(cols[2])  if cols[2] else None,\n", "            int(cols[3])  if cols[3] else None,\n", "            int(cols[4])  if cols[4] else None,\n", "            int(cols[5])  if cols[5] else None,\n", "            int(cols[6])  if cols[6] else None,\n", "            int(cols[7])  if cols[7] else None,\n", "            cols[8]       if cols[8] else None,\n", "            cols[9]       if cols[9] else None,\n", "            cols[10]      if cols[10] else None,\n", "            int(cols[11]) if cols[11] else None,\n", "            int(cols[12]) if cols[12] else None,\n", "            int(cols[13]) if cols[13] else None,\n", "            int(cols[14]) if cols[14] else None,\n", "            int(cols[15]) if cols[15] else None,\n", "            cols[16]      if cols[16] else None,\n", "            cols[17]      if cols[17] else None,\n", "            int(cols[18]) if cols[18] else None,\n", "            int(cols[19]) if cols[19] else None,\n", "            int(cols[20]) if cols[20] else None,\n", "            cols[21]      if cols[21] else None,\n", "            cols[22]      if cols[22] else None,\n", "            cols[23]      if cols[23] else None,\n", "            int(cols[24]) if cols[24] else None,\n", "            int(cols[25]) if cols[25] else None,\n", "            int(cols[26]) if cols[26] else None,\n", "            int(cols[27]) if cols[27] else None,\n", "            int(cols[28]) if cols[28] else None\n", "         ))             \n", ")\n", "    \n", "# create dataframe\n", "df = sqlContext.createDataFrame(cleaned_data_to_columns, airline_data_schema)\\\n", "    .select(['year', 'month', 'day_of_month', 'day_of_week',\n", "            'scheduled_departure_time','scheduled_arrival_time',\n", "            'arrival_delay', 'distance', \n", "            'src_airport', 'dest_airport', 'carrier'])\\\n", "    .cache()"], "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 20, "cell_type": "code", "source": ["print(df.dtypes)\n", "df.describe().show()"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["## 4.3. Night flight example\n", "Using the contructed DataFrame, we can answer the questions about night flights from the previous section:\n", "\n", "- How many night flights do we have in our data?\n", "- How many night flights per unique carrier?"], "cell_type": "markdown", "metadata": {}}, {"execution_count": 21, "cell_type": "code", "source": ["df[df.scheduled_departure_time > 1800].count()"], "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 22, "cell_type": "code", "source": ["df[df.scheduled_departure_time > 1800].groupBy(df.carrier).count().orderBy('count', ascending=0).collect()"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["![](https://farm2.staticflickr.com/1604/24934700445_833f0a5649_t.jpg)"], "cell_type": "markdown", "metadata": {}}, {"source": ["<div class='anchor' ></div>\n", "## Question 6\n", "\n", "\n", "### Question 6.1\n", "Using Spark SQL, calculate how many flights have a scheduled departure time after 09:00 and before 14:00."], "cell_type": "markdown", "metadata": {}}, {"source": ["```python\n", "flights = df[(df.scheduled_departure_time > ...) & (df.scheduled_departure_time < ...)]\n", "flights....\n", "```"], "cell_type": "markdown", "metadata": {}}, {"source": ["<div class='anchor' ></div>\n", "### Question 6.2\n", "Compute the number flights that have scheduled departure time after 09:00 and before 14:00, for each source airport (origin). Plot top 5 of them."], "cell_type": "markdown", "metadata": {}}, {"source": ["```python\n", "top5_source_airport = flights.groupBy(...).count().orderBy('count', ascending=0).take(5)\n", "\n", "pdf = pd.DataFrame(data=top5_source_airport)\n", "\n", "print(pdf)\n", "\n", "%matplotlib inline\n", "import matplotlib.pyplot as plt\n", "\n", "\n", "# create `virtual indexes for carriers which will be used as x-values`\n", "airport_indexes = range(0, len(top5_source_airport))\n", "\n", "# plot\n", "plt.bar(airport_indexes, pdf[1], align=\"center\")\n", "\n", "# put x-labels for the plot\n", "plt.xticks(airport_indexes, pdf[0])\n", "plt.show()\n", "```"], "cell_type": "markdown", "metadata": {}}, {"source": ["# Summary\n", "\n", "In this lecture, we gained familiarity with the Jupyter Notebook environment, the Python programming language and its modules. In particular, we covered the Python syntax, Numpy - the core library for scientific computing, Matplotlib - a module to plot graphs, Pandas - a data analysis module. Besides, we started to gain practical experience with PySpark and SparkSQL, using as an example a dataset concerning US flights."], "cell_type": "markdown", "metadata": {}}, {"source": ["# References\n", "This notebook is inspired from:\n", "\n", "- [Python Numpy tutorial](http://cs231n.github.io/python-numpy-tutorial/)"], "cell_type": "markdown", "metadata": {}}], "metadata": {"kernelspec": {"display_name": "Python 3", "name": "python3", "language": "python"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "name": "python", "file_extension": ".py", "version": "3.5.2", "pygments_lexer": "ipython3", "codemirror_mode": {"version": 3, "name": "ipython"}}}}